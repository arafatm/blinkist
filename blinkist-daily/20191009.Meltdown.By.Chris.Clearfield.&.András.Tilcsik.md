# Meltdown
*by Chris Clearfield & András Tilcsik*

Source: [https://www.blinkist.com/books/meltdown-en-chris-clearfield-and-andras-tilcsik](https://www.blinkist.com/books/meltdown-en-chris-clearfield-and-andras-tilcsik)

![Meltdown](https://images.blinkist.com/images/books/5d7b551c6cee0700095efd73/3_4/470.jpg)

 (2018) unpacks the weaknesses shared by local and global systems. With diverse and astonishing examples, it provides empowering solutions to avoid failure. The 


# What’s in it for me? Facts, failure-proofing and maybe even preventing the next apocalypse.

When you hear the word **meltdown**, what comes to mind – an accident in a nuclear reactor, maybe? Once you’ve been through these blinks, you’ll also be able to call on dozens of examples of specifically modern kinds of meltdown, their causes and how to prevent the same happening to you or your organization.

We’re living in an age of unprecedented technical capability: transport, commerce, medicine, power… you name it, the systems around us are more advanced and yet also more complex than ever before. That’s why authors Chris Clearfield and András Tilcsik wrote **Meltdown**, to unpack the ways complexity leads to failure. And they’re making the antidotes available to us all.

In these blinks, we’ll cover the core components that lead to modern system failure and how to tackle them. Then we’ll break down the tools needed to failure-proof systems and organizations – like structured decision-making, diversity, dissent, reflection, iteration and a list of warning signs.

In these blinks, you’ll learn

- how the Fukushima nuclear disaster could have been avoided;
- why diversity makes organizations stronger; and
- how parents can learn from ER teams and harness the power of iteration.

# Modern systems often fail for similar reasons in very different contexts.

What do BP’s oil spill in the Gulf of Mexico, the Fukushima nuclear disaster and the global financial crisis all have in common? Yes, they’re all crises, but they also share the same underlying causes.

Modern systems are more capable than ever, yet increased capability has also driven up complexity and made systems less forgiving. Take the finance industry, for instance: the switch from face-to-face to computerized stock-market trading has helped reduce operational costs, increased trading speed and given more control over transactions. But the digital shift has also made the system harder to understand and increased the chance of complex, unexpected interactions. Finance has become a perfect example of what sociology professor Charles Perrow would call a **complex**, **tightly coupled** system. 

Perrow was an expert on organizations hired in the late 1970s to investigate the causes of a nuclear accident in Pennsylvania. What he discovered revolutionized the science of catastrophic failure. 

Perrow identified a combination of small failures behind the disaster that interacted in a domino effect. Instead of blaming the nuclear plant’s operators or calling it a freak occurrence, Perrow saw that the accident had been caused by features inherent in the plant as a system – complexity and tight coupling. 

Tight coupling is an engineering term for when a system is unforgiving or has little buffer between its parts. The system for cooking a Thanksgiving dinner, for instance, is tightly coupled: the meal involves many elements that depend on each other, like stuffing that cooks inside the turkey and gravy that comes from the roasted bird’s juices. And with only one oven in most houses, one dish could set back all the rest.

Complexity in a system means that it’s non-linear and hard to see inside. If we stick with the Thanksgiving dinner analogy, cooking a turkey is a complex system, because it’s hard to see inside the bird to tell if it’s cooked. Complexity in a system makes it tough to identify problems and their knock-on effects.

A combination of complexity and tight coupling takes us into what Perrow calls the **Danger Zone**. This is where meltdown – the collapse or breakdown of a system – becomes highly likely. So that tightly coupled, complex Thanksgiving dinner could well be doomed unless precautions are taken.

Perrow’s **complexity/coupling **formula reveals the shared DNA behind all kinds of modern meltdowns, so failure in one industry can now provide lessons in other fields. We’ll find out how in the following blinks. 

# Failure can be avoided by reducing complexity and increasing the buffer between the parts of any system. 

When you go for a drive you put on a seatbelt even though you don’t know the exact nature of any accident that could happen. You simply know the danger is there and that the seatbelt could potentially save your life. In the same way, Perrow’s complexity/coupling formula helps prevent failure without showing the exact form it could take.

This means that you can plan ahead to reduce complexity, for instance by increasing the transparency of a system. Failure to do so can even lead to fatal accidents. Take 27-year-old **Star Trek **actor Anton Yelchin, who, in 2016, died getting out of his Jeep Grand Cherokee when the vehicle rolled and pinned him against a brick pillar. 

The reason behind this tragic event was the design of the car’s gearshift. It was elegant but it didn’t clearly show whether the car was in “park,” “drive,” or “reverse.” In other words, the system was unnecessarily opaque and complex, which led to Yelchin wrongly assuming the vehicle would remain still. Tragedy could have been avoided had the gearshift been designed more transparently and indicated clearly in which mode the Jeep was. 

Transparency reduces complexity, making it hard to do the wrong thing – and easier to realize if you’ve made a mistake.

Sometimes, though, transparency isn’t possible. If you think of an expedition to climb Mount Everest, there are dozens of hidden risks from crevasses and falling rocks to avalanches and sudden weather changes. The mountain is always going to be an opaque system. So mountaineering companies troubleshoot small problems, like delayed flights, supply problems and digestive ailments **before** they can accumulate into major crises. This stops such problems from hindering the final climb, where there’s little margin for error.

When the complexity won’t shift, there’s always the **buffer.**

Nuclear engineer turned management consultant, Gary Miller, describes how he saved a bakery chain from failed expansion by increasing their buffer. Before rollout, he spotted that their new menu was overly complex and relied on an intricate network of suppliers. When they refused to address this, Miller persuaded them to relax their aggressive launch schedule instead, which allowed them enough slack to deal with problems when they inevitably surfaced. 

Perrow’s complexity/coupling formula helps figure out **if** a project or business is vulnerable to failure and **where.** It identifies vulnerabilities in a system, even if it can’t tell you exactly what will go wrong. As Miller says: “You don’t need to predict it to prevent it.”

# Using structured decision-making tools can help you avoid disasters big and small.

We often go through life making snap judgments or using our instincts. There’s no harm in that until we find ourselves operating in a complex system.

Engineers of the Fukushima Daiichi nuclear plant in Japan used their instincts when they falsely estimated the height of their tsunami defense wall. On March 11, 2011, an earthquake caused a wave several meters higher than they had planned for, which flooded the generators responsible for cooling and triggered the world’s worst nuclear accident in 25 years.

But how could the engineers have done any better?

Defenses of this kind are vast and expensive to build, and as they couldn’t build an infinitely tall wall, the engineers had to anticipate a height that they were **very sure **would work. To do this, they used a **confidence interval**, a calculation based on comparing the highest probable wave height and the lowest. The trouble is that humans are not very good at making these kinds of forecasts: the ranges we draw are too narrow. 

One solution is to use a structured decision-making tool called **SPIES**, or **Subjective Probability Interval Estimates**. It pushes us to consider a broader range of outcomes. Instead of just assessing the best and worst plausible scenarios, SPIES estimates the probability of several outcomes within the entire range of possibilities. It’s not perfect, but studies consistently show that this method hits the correct answer more frequently than other forecasting methods. Had the Fukushima engineers used SPIES, they could have safeguarded against overconfidence and been less likely to overlook the seemingly implausible scenario that flooded their defenses. 

Another structured decision-making tool is the use of **predetermined criteria**, which helps us focus on the factors that really matter. Take the** Ottawa Ankle Rules**, for example: this set of predetermined criteria was developed in Canada in the early 1990s and to reduce doctors’ use of unnecessary X-rays of feet and ankles by a third. By focusing only on pain, age, weight-bearing and bone tenderness to decide whether an X-ray was necessary, they avoided getting side-tracked by irrelevant things, like swelling.

In complex systems like medicine and tsunami prediction, the effects of our decisions are hard to understand or learn from, and intuition often fails us. That’s when tools like SPIES and predetermined criteria can provide an interruption to business as usual, allowing us to approach our choices systematically.

# Complex systems give off warning signals that we can use to save lives, money and reputations.

We often choose to ignore warning signs – if your toilet blocks, for instance, would you simply consider it a minor inconvenience, or see it as a warning sign of an impending flood? 

Ignoring warning signs can sometimes have catastrophic consequences. In Washington DC in 2005, three metro trains came within a few feet of crashing deep under the Potomac River. Only luck and quick action by the train drivers saved the day. Engineers suspected that the underlying cause was a problem with the track sensors, but before they got around to fixing it, the problem went away. So they invented a testing procedure, hoping to ensure the same glitch couldn’t happen again elsewhere. The trouble was, their bosses soon forgot about this near-miss and stopped running the tests. Four years later, the same error showed up in a different spot, causing a horrific crash and the deaths of nine people.

We often ignore the clues in small errors, as long as things turn out OK. That near-disaster in 2005 was a warning sign that the metro organization chose to ignore. An essential feature of complex systems is that we can’t find all the problems just by thinking about them. Luckily, before things fall apart, most systems give off warnings.

Unlike the DC metro, the commercial airline industry is a prime example of how paying attention to small errors can pay off. By doing so, they have collectively reduced fatal accidents from 40 in every one million departures to two in every **ten million **over the past 60 years in a process called **anomalizing**. 

Here’s how it works:

First, data needs to be gathered on all flights. Next, the issues need to be raised and fixed – incident reports shouldn’t gather dust in a suggestion box. The third step is to understand and address the root causes instead of seeing mistakes as a series of isolated incidents. If pilots on a particular route keep flying dangerously low, then there could be an underlying signal or signage problem, for instance.

The final step is to share learnings, which sends a clear message that mistakes are normal and helps colleagues anticipate issues that they might one day face. 

In systems like air and metro travel, we can learn from specific operational incidents. Business owners, on the other hand, can learn from a dedicated team or a trusted adviser hired to identify threats from competitors, technological disruptions and regulatory changes.

# Encouraging dissent makes teams more effective and systems stronger.

Speaking up isn’t easy. In fact, neuroscience shows us that a desire to conform is hard-wired into our brains. But that doesn’t make dissent any less valuable. Here’s why:

In a strange-but-true study of airline crew errors, the US National Transportation Safety Board (NTSB) found that between 1978 and 1990 nearly **three-quarters** of major accidents happened when it was the captain’s turn to fly. That is, not the less-experienced first officer. That was alarming because the captains were flying 50 percent of the time, so their errors should have been equal to or less than their deputies’. 

So the NTSB dug deeper.

They confirmed that the captains weren’t worse at their jobs – far from it – but that their seniority meant their mistakes were going unchallenged. Their first officers lacked the tools with which to give the captain feedback and were bottling up concerns or giving vague hints instead of raising alarms. Hierarchy was putting lives in danger.

So the airlines introduced a groundbreaking training program called **Crew Resource Management **(**CRM**). The pilots thought it so basic that they jokingly called it **charm school, **but it broke the taboo around raising concerns. CRM radically reduced the number of accidents **and **leveled responsibility to 50:50 between pilots and their deputies. By democratizing safety, the program empowered everyone from cabin crew to baggage handlers to voice concerns, also harnessing the motivational power of shared responsibility. 

So how do you encourage dissent in other types of organizations?

One effective way is through **open**, as opposed to **directive** leadership. **Directive leaders** will state their own preferred solutions at the start of a conversation and tell colleagues that the goal is to all come to an agreement. 

An **open leader **will hold back their own opinion until last and encourage colleagues to discuss as many perspectives as possible. Amazingly, this simple technique is proven to yield more possible solutions and almost twice as many facts, which makes for a better-informed discussion. Simple!

If hierarchies, social concerns and even our brains’ wiring work against dissent, then it’s paramount for leaders to nurture more than just an open-door policy. As dissent expert Jim Detert explains, you need to actively encourage people to speak up, or else you’ll be discouraging them.

# Building diverse teams helps reduce risk and improve results for organizations.

Most people agree that diversity is a fair and positive thing, right? But did you know that it’s also been shown to reduce risk for organizations?

In a landmark 2014 study, scientists proved the benefits of ethnic diversity when it comes to decision-making. In a simplified stock-market simulation, they observed dozens of diverse and homogenous groups as far afield as Singapore and Texas and assessed the accuracy of their trading. And guess what? The diverse groups performed markedly better than homogenous ones, pricing stocks more accurately and making fewer mistakes. Fascinatingly, the study found that crashes were more severe and price bubbles more frequent in homogenous markets because homogenous groups put too much faith in each others’ decisions, which caused errors to multiply. In diverse markets, participants were more critical of each others’ decisions and copied less often, which led to more rational decision-making. 

In hindsight, we can apply the lessons of this study to understand the financial crash of 2007 and 2008. Former Citigroup CFO Sallie Krawcheck stated in a 2014 interview that those responsible for the crash weren’t “a bunch of evil geniuses” able to foresee the financial downturn, but that they **were** “peas in a pod.” Krawcheck blamed the lack of diversity for the poor decisions leading up to the crash and argued that diversity makes it more permissible to ask questions without looking stupid or worrying you’ll lose your job.

So why don’t all companies have compulsory diversity schemes? They must be effective, right?

Wrong. In a 2016 **Harvard Business Review **paper, sociologists Frank Dobbin and Alexandra Kalev found that the most frequently used diversity programs failed to get results. Not only that, but over the past three decades and in more than 800 US firms, compulsory diversity schemes actually made organizations **less diverse. **They found that managers had rebelled against compulsory schemes because they felt they were being policed. They had resisted hiring diversely just to assert their autonomy.

Luckily there are other solutions that work. Over the same three decades, voluntary as opposed to compulsory mentoring schemes were proven successful in helping diverse candidates progress. These schemes naturally reduced bias with positive messaging: managers felt like they were being exposed to new talent pools rather than having their hiring decisions policed. The study also found that formal mentoring schemes were more effective than informal ones because white male executives didn’t feel comfortable approaching young women and minority men informally. Allocating them mentees removed this awkwardness and allowed them to mentor a diverse range of junior employees. 

With benefits like increasing accuracy and avoiding the next financial crash, diversity is the safe alternative to homogeneity and groupthink. The healthy skepticism that comes with diversity makes organizations and decisions stronger.

# Reflection and iteration are essential coping strategies for high-pressure scenarios.

We’ve all been there – with the end of a project in sight there’s a strong urge to rush to the finish, even if the conditions have changed. 

Pilots call it **get-there-itis, **but the technical term is **plan continuation bias**, and it’s an alarmingly common factor in airline accidents. If you’re only 15 minutes away from your destination, and the weather changes, it’s much harder to divert to a nearby airport than when you’ve just started out. Perhaps you’ve experienced the same effect when working toward a deadline? 

Pilot Brian Schiff knew the dangers of get-there-itis when he refused to take a furious Steve Jobs on his charter flight. In spite of pressure and entreaties, Schiff remembered his training and calculated that hot weather, heavy luggage and hilly terrain would make take-off in their small plane risky. Schiff still remembers how intimidated he felt as a puny 20-year-old in the firing line of Jobs’ wrath. But he didn’t budge and refused to fly. Schiff stood up to a very important customer, but instead of being reprimanded, he was rewarded for pausing and prioritizing safety in a high-pressure situation, and saw his pay doubled. 

But sometimes there’s no time for reflection when situations change: in an emergency room, for instance, medics have to balance caregiving tasks like resuscitation and administering medication with monitoring the patient’s overall condition. This is where an **iterative** process becomes indispensable.

Effective iteration includes three basic steps: tasks, monitoring, then diagnosis – or offering a solution. 

The three steps are then repeated in a cycle to evaluate and improve solutions on the go. 

A great domestic example of this is a parenting strategy from the authors of a paper called “Agile Practices for Families: Iterating with Children and Parents.” To improve the chaos of their family’s morning routine, the Starrs decided to hold regular family meetings to discuss what had gone well that week, what they could improve and what they would commit to improving in the week to follow.

After committing to changes, they would repeat the questions at the next meeting, allowing them to focus on the most effective solutions over time. Their experiment was so successful that when **New York Times **columnist Bruce Feiler visited their home, he described theirs as “one of the most astonishing family dynamics I have ever seen.” 

You can use iteration to check in whenever you have a backlog of tasks and deadlines. The key is to go through the steps and then **re-evaluate **once you’ve tried a solution.

# Final summary

The key message in these blinks:

**We live in the golden age of meltdowns, but it’s within our reach to bring that era to an end. The solutions listed in these blinks are hard to implement because they often go against our natural instincts or accepted organizational and cultural norms. But if we give due consideration to complexity and tight coupling, we can unlock greater innovation and productivity in modern systems while avoiding catastrophic failure.**

Actionable advice:

**Give yourself a pre-mortem.**

You might have heard of a post-mortem, but did you know that its inverse – **a pre-mortem** – can help to prevent failure? 

When planning a project, try imagining failure as a foregone conclusion. Research shows that you’ll then think of far more potential glitches than if you’d imagined what success would look like. This technique harnesses what psychologists call **prospective hindsight**, and it’s also useful for finding more concrete and precise reasons for an outcome.** **

So next time you’re at the planning stage, instead of asking, “how can we make this work?” try “what could have caused this to fail massively?”

**Got feedback?**

We’d sure love to hear what you think about our content! Just drop an email to remember@blinkist.com with the title of this book as the subject line and share your thoughts!

**What to read next: ******Originals******, by Adam Grant**

In these blinks, you’ve learned why even the most well-crafted systems sometimes fail. But you’ve also learned some of the things we can do to make sure we catch the cracks in the system before disaster strikes. One important takeaway is that every team needs to have its share of non-conformists, people who can think outside the box and who aren’t afraid of criticizing the status quo.

But non-conformity and the ability to think differently aren’t just important in avoiding the meltdown of systems. In **Originals**, author Adam Grant argues that non-conformity and creativity are indispensable for anyone who wants to succeed in life. But how do you encourage creativity and non-conformity in yourself? Grant has some good advice. So if you want to give your creativity a boost, head over to our blinks to **Originals**.
