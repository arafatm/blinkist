# The Reality Game
*by Samuel Woolley*

Source: [https://www.blinkist.com/books/the-reality-game-en](https://www.blinkist.com/books/the-reality-game-en)

![The Reality Game](https://images.blinkist.com/images/books/5f482d9d6cee070006d18d96/1_1/470.jpg)

 (2020) sheds light on the murky world of “computational propaganda” – political manipulation using digital tools. Samuel Woolley argues that fake news, viral conspiracy theories, and Twitter bot armies don’t just sow confusion and discord; in his view, they also subvert the democratic process. That means it’s high time we fought back and reclaimed our digital space from today’s unaccountable mega-platforms. 


# What’s in it for me? Join the fight against fake news.

Social media has been weaponized. Authoritarian governments around the world control armies of bots and fake accounts. Their purpose? More often than not, it’s to harass journalists and sow doubt about the work of dissidents.

But it’s not just dictators who use digital weaponry; democracies aren’t immune, either. In the 2016 presidential election in the United States, Donald Trump’s team created thousands of fake social media accounts to push his preferred talking points. 

And across the Atlantic, the British campaign for leaving the EU deployed similar tactics. Both Trump and the Brexiteers rode the wave of misinformation all the way to victory.

But efforts to dupe and deceive aren’t just about technology. Underneath the torrent of fake news and the manipulation is a complex web of social, economic, and political problems. 

This means that the real issue isn’t social media itself, which can be a force for good, but the **misuse** of social media. The real issue, then, is how we can stop the misuse of digital tools. The first step is to figure out why social media is so dysfunctional. 

In these blinks, you’ll also find out 

- why bots aren’t nearly as smart as you might think; 
- how fake news capitalizes on people’s desire to think deeply; and 
- why unlimited free speech is not the answer to social media’s problems.

# Old media helped to shore up faith in institutions; new media undermines it.

Trust in democratic institutions is at an all-time low. A 2018 Gallup poll found that, over the last four and a half decades, the number of Americans who trust Congress plummeted from 43 to 11 percent. Confidence in banks halved, and only 38 out of 100 people say they trust religious establishments – down from 65 in the early 1970s. 

But the problem isn't just limited to the United States. Similar trends have been registered in Brazil, in Italy, and in South Africa – countries we’ve always considered democratic. So what’s going on? Why have citizens become so distrustful of their democratic institutions? Well, it all comes down to how we use the media to understand reality. 

**The key message in this blink is: Old media helped to shore up faith in institutions; new media undermines it. **

The internet has changed the media landscape beyond recognition. Before the digital age, information flowed in a single direction. One person spoke; many listened. Think about TV presenters or newspaper columnists – individuals who addressed thousands of listeners and readers. Respected news anchors and journalists made an effort to be objective. Before a view was aired or published, it had to be vetted. Of course, conspiracists were free to write to the **New York Times**, but the paper was also free to decide whether or not to publish such letters. 

This model meant that millions of citizens received their information from the same sources. This helped build consensus on what was real and true and, as a result, created trust. 

But in the last few years, things have changed. Information is now shared via what’s called the “many-to-many” paradigm. You no longer have to persuade anyone to print or broadcast your views. On the web, you are your own publisher – and anyone can read what you post. 

Internet pioneers were hoping for a golden age of free speech and civic participation. But what they didn’t foresee was that the landscape would become controlled by just a few giant social media corporations, each less accountable than old-school broadcasters.

There is little or no regulation in the world of social media, so it has become a breeding ground for disinformation. And in this murky mess of easily gamed algorithms, it’s difficult to tell fact from fiction. 

A 2018 poll in the UK, for example, found that 64 percent of Brits struggled to tell the difference between real and false news. No wonder, then, that trust is at an all-time low – in this environment, it’s often impossible to know whom to trust!

# Often driven by commercial motives, fake news effectively appeals to people’s desire to think critically.

“FBI AGENT SUSPECTED IN HILLARY EMAIL LEAKS FOUND DEAD IN APPARENT MURDER-SUICIDE.” 

In early November 2016, this sensational headline – printed in all caps – appeared on a website called the **Denver Guardian**, which billed itself as “Colorado’s oldest news source.” The story implicated the Democratic presidential candidate, Hillary Clinton, in a scandalous cover-up. 

Before long, thousands were clicking. At one point, the story was shared by more than 100 Facebook pages every minute. There was just one problem: the revelation, like the paper that broke it, was bogus. 

**The key message in this blink is: Often driven by commercial motives, fake news effectively appeals to people’s desire to think critically. **

Nothing about that **Denver Guardian** story was true. The article had been written by Jestin Coler, an entrepreneur from California. He owned the website, and his publication’s only purpose **[PAUSE] **was** **to generate ad clicks. The easiest way of attracting traffic, Coler realized, was to spread false reports about the ongoing presidential contest.

It was a profitable business. In the run-up to the November election, junk news brought Coler between $10,000 and $30,000 a month. 

“The people wanted to hear this,” Coler later stated. He openly admitted that he’d invented every single detail in that story. It was all fiction – the town he mentioned, the people, the sheriff, the FBI agent. Once he’d written the piece, Coler said, his social media team planted it on pro-Trump forums. Within hours, it was spreading like wildfire. 

Coler’s motives were purely commercial, and that made him different from people who’d run better-known disinformation campaigns. But his viral story fed into a larger digital attack on the truth. That offensive involved bogus ads on Facebook – paid for by the Russian government – as well as fake stories peddled by teens in Moldova and “political action groups” linked to the Trump camp. 

Coler was right, though – people do want to hear these stories. Why? Well, many still hanker for the kind of news that was once put out by trusted broadcasters. That’s why the **Denver Guardian** emphasized its status as Colorado’s oldest news source. 

People also want to get to the root of things. Vetted and fact-checked investigative journalism used to answer this need. But as it faded away, peddlers of disinformation – who have no qualms faking provenance of news or disguising wild speculation as critical thought – came to the fore. 

# Social media’s hands-off approach to monitoring speech is a boon to conspiracists.

There’s nothing new about conspiracy theories. People have always tried to second-guess those who rule over them. In every society, there have been dark rumors of plots, scandals, and cover-ups. But in recent decades, something shifted.

In the past, conspiracies spread slowly; rumors about the evil doings of kings and presidents, impossible to confirm, rarely made it onto the evening news. But the world of social media is different. Online, conspiracies spread faster and reach farther. 

**The key message in this blink is: Social media’s hands-off approach to monitoring speech is a boon to conspiracists. **

Take QAnon, a conspiracy theory associated with the far right in the United States. Its supporters claim there is a cabal of unelected government officials known as the “deep state” that is, allegedly, working against Donald Trump. 

Conspiracists also believe that an anonymous patriot called “Q” has infiltrated this cabal. This nickname is a reference to the highest level of security clearance available to American officials. Posts signed by Q occasionally appear on forums like 4chan. Q has claimed, for example, that liberal politicians were running a child sex ring out of a pizza joint’s basement.

Q’s claims spread quickly. Coordinated groups of far-right activists pick them up and seed them on bigger sites, like Reddit and Twitter. They do this with the help of social media “bots” – software that automates tasks like content-sharing. Bots trick algorithms into boosting posts whose popularity seems genuine. After a while, regular people start to pick up on the conspiracy. This becomes a story for traditional media, which amplifies things even further. 

The problem is that these conspiracies corrode democratic norms. In QAnon’s telling, liberals are committed to nothing less than subverting American values. If you accept this idea, of course you’d think they have no right to govern the country – even if they do win elections. 

So why don’t social media companies like Facebook and Twitter stop these conspiracies? Well, they don’t believe that it’s their role to monitor the speech of their users. And, in any case, people do have a right to free speech, don’t they?

Social media companies have also been slow to stop the use of bots that push conspiracy theories. The upshot? An environment that can be gamed by fringe conspiracists. And, as we’ll see in the next blink, mainstream political actors are also learning to benefit from it. 

# The first known case of bots interfering in the political process occurred in the United States.

In late 2013, Ukrainians took to the streets to protest against the country’s pro-Russian president, Viktor Yanukovych. The Russian government, which saw Mr. Yanukovych as a long-standing ally, responded with an online offensive. Thousands of Russian-operated social media accounts and bots flooded the web. They spread disinformation directed against the protestors. A similar strategy would later be used against the United States in the 2016 presidential election.

At the time, many onlookers believed they were witnessing the birth of computational propaganda. But while the Russian government may have mastered the dark art of digital subversion, it wasn’t the first political actor to deploy these tactics. 

**The key message in this blink is: The first known case of bots interfering in the political process occurred in the United States. **

In 2010, voters in Massachusetts headed to the polls to elect a new senator. Two names were on the ballot: Scott Brown, the Republican candidate, and Martha Coakley, a Democrat. 

Coakley was a clear favorite. Massachusetts had been a liberal stronghold for decades. Ted Kennedy, the Democratic senator whose sudden death had triggered the contest, had represented the state since 1962. Suddenly, though, Brown’s popularity began to surge. 

And this was when computer scientists at Wesleyan University noticed something odd. Suspicious-looking Twitter accounts appeared to be engaged in a coordinated attack on Coakley. The accusation? That the Democratic candidate was anti-Catholic – a serious charge in a state like Massachusetts. 

These accounts had no biographical data or followers besides each other, and they only ever posted about Coakley. They attacked her at regular 10-second intervals. It was clear that these were bots, but who was driving them? 

The researchers eventually traced the accounts back to a group of tech-savvy conservative activists based in Ohio, a state that’s miles away from Massachusetts. But the fake personas created by those activists were designed to look like concerned locals. A cursory glance would have told you that attacks were coming from ordinary people in Coakley’s own state – not from bots driven by cross-border politicos. 

Worryingly, this bet paid off. The bots generated so much noise that mainstream outlets like the **National Catholic Register** and **National Review** picked up on the rumors that Coakley was anti-Catholic. The stories they ran pointed to the bots’ activity on Twitter. Journalists mistook the work of bots for a genuine grassroots movement. Outlets that published these stories unwittingly amplified a manufactured scandal. Brown pulled off an upset. Coakley lost the race.

# Bots are dumb, but that doesn’t mean they’re not effective.

Every time you log onto Twitter and look at popular posts, you’re likely to encounter bot accounts. They’re busy liking, sharing, and commenting on other users’ content. It’s not hard to spot messages produced by this kind of software. Whether it’s spam or political vitriol, bots usually “write” garbled prose with wonky syntax. Their posts are precisely timed and come in machine-like bursts.

In other words, your average bot isn’t very sophisticated. So why, then, do so many journalists say that bots have “conquered democracy?” One answer is that it’s easier to focus on bots than to ask the more disquieting question: Why is it so easy to hack democracy with such simple tools? We’ll come back to that latter issue, but first, let’s take a closer look at these bots.

**The key message in this blink is: Bots are dumb, but that doesn’t mean they’re not effective. **

Take a moment to rewind back to 2016’s presidential contest between Hillary Clinton and Donald Trump. 

There was a lot of hype surrounding the role bots played in that election. Cambridge Analytica, the political consulting firm that advised the Trump campaign, promised to launch a whole army of them. The company claimed that its intelligent bots would target key demographics with laser-focused pro-Trump and anti-Clinton talking points. To achieve this, Cambridge Analytica was going to leverage what’s called “psychographic” data – that is, detailed profiles collated from voters’ social media pages.

This tool is extremely sophisticated. But all the evidence we have suggests that it was never used. Trump’s camp did run an extraordinarily effective digital campaign, though. And that underscores the most important point here. When it comes to computational propaganda, you don’t have to be smart to win. 

Take it from the Computational Propaganda Project at Oxford University. Its research has seen the same story play out time and again. From Russia’s digital offensive against Ukraine in 2013 to the Brexit referendum and Trump’s presidential campaign in 2016, the vast majority of bots that have been used to spread disinformation have been very simple. All they could do, in fact, was like or reshare content, spread links, and troll people. These bots weren’t functionally conversational. They possessed no artificial intelligence. 

But the little they could do was enough to overwhelm their targets. Their cascades of repetitive talking points were so prolific that opponents found themselves swamped, unable to respond quickly or effectively.

# 

“**Complex mechanisms like artificial intelligence have played little role in computational propaganda campaigns to date.”**

# Social media companies should self-regulate, but they do not take responsibility for the misuse of their tools.

As we’ve learned, you don’t need sophisticated technology to hack public opinion. Whether you run a political campaign, or simply hawk junk news, all you need to overwhelm social media algorithms are rudimentary bots and clickable headlines. 

This brings us back to the disquieting question we posed in the last blink: Why is democracy so vulnerable to simple hacking tools? One answer can be found in the way social media companies are regulated. In public life, these corporations now occupy the position once held by newspapers and broadcasters. But the rules governing their work are far softer. 

**The key message in this blink is: Social media companies should self-regulate, but they do not take responsibility for the misuse of their tools. **

Take the Federal Election Commission. This regulatory body enforces the law about political campaign finance. In 2006, the Commission decided that online campaigning wasn't its responsibility. To be fair, they did make an exception for political ads. But the rise of viral fake news and bot-driven digital disinformation was never going to be on the radar of America’s Election Commission. 

So that regulatory tool is out. If there anything else the federal government can use? Well, there’s Section 230 of the Communications Decency Act. This statute, passed in 1995, grants internet corporations the right to censor harmful speech. It also exempts them from any responsibility if they do get things wrong. 

Social media companies in America do use Section 230. It helps them justify decisions to delete obvious or violent hate speech, such as anti-Semitic neo-Nazi diatribes. But corporations also treat this law as a license to ignore problematic political content like disinformation. As the executives of these companies see it, the statute confirms their own view that it is not up to them to act as “arbiters of truth.” In fact, Section 230 does grant these companies the power to arbitrate content, but they rarely use it. Why? One possible explanation is that it conflicts with the libertarian ethos of Silicon Valley. 

But even if companies like Facebook and Twitter did use this power, cleaning up their platforms would be a daunting task. These sites grew extremely quickly – and without much consideration for ethical design. To prevent future misuse, you’d have to bolt ad hoc fixes onto a structure that was not created to accommodate them. As a Facebook employee put it in an interview with the author, the social media giant is like a plane that’s been launched into flight despite only being half-built.

# 

**“Multibillion-dollar corporations, feckless governments, and technology investors are primarily to blame for the rise of computational propaganda.”**

# Machine learning can help us deal with digital disinformation, but it’s not enough on its own.

Let’s ask one final question. Could we simply ban bots? In 2018, American senator Dianne Feinstein attempted to do just that by proposing the “Bot Disclosure and Accountability Act.” 

But the bill stalled in Congress. It’s hard to implement anti-bot measures without also violating the constitutional right to free speech. And there’s more. You may legislate against bots, but that won’t solve disinformation’s other problem: humans spread false news just as effectively as bots. The Chinese “50 Cent Army,” for example, is a coordinated troop of human accounts responsible for inundating the web with pro-government propaganda. Their activities would not be covered by anti-bot laws. 

So we might be better off with a different focus. Perhaps our target shouldn’t be the bots themselves; instead, maybe we should think more about information – and how it flows. 

**The key message in this blink is: Machine learning can help us deal with digital disinformation, but it’s not enough on its own. **

Imagine a bot that uses machine learning. This program would learn from conversations with people on social media. It would constantly get better at talking to real humans. 

Now imagine this program started chatting to someone and convinced them to share an article dismissive of global warming. That conversation alone would generate lots of data. The bot would then analyse the data to figure out which tactics worked and which didn’t. And this analysis would inform the bot’s future behavior. 

Pretty nightmarish, right? Sure, but this isn’t a one-way street. Machine learning can also be deployed to combat digital disinformation. Take it from the researchers at the Observatory on Social Media at Indiana University who built the “Botometer.”

The Botometer uses machine learning to classify Twitter accounts as human or bot. It does this by analyzing more than a thousand features of any given account. The software looks at its friends, activity pattern, language, and tone of posts. At the end of this analysis, the Botometer provides an overall “bot score.” It tells users whether they are interacting with a human or a bot, immediately highlighting one of the most common sources of disinformation. 

But machine learning algorithms like the Botometer are probably not enough on their own. Most experts predict the emergence of hybrid – or “cyborg” – regulatory models. For example, humans could fact-check stories, and automated systems might be employed to quickly identify the bots who spread fake news. 

Some tasks, it seems, are just too important to be entrusted to our robot overlords. 

# Final summary

The key message in these blinks:

**The media landscape has changed beyond recognition. In the early days of the internet, many celebrated the eclipse of gatekeeping institutions like newspapers and broadcasters. But allowing people to pick their own news sources didn’t boost civic participation – it ate away at trust and created fertile conditions for digital disinformation. This problem has been exacerbated by light-touch regulation and t****he refusal of social media companies to tackle bots. ****But machine learning, used alongside human fact-checking, might help us win this battle. **

**Got feedback?**

We’d love to hear what you think about our content! Just drop an email to [[email protected]](/cdn-cgi/l/email-protection) with The Reality Game as the subject line and share your thoughts!

**What to read next: ******Tools and Weapons******, by Brad Smith and Carol Ann Browne**

As we’ve seen in these blinks, social media companies bear a great deal of responsibility for the current state of our digital space. Few of these companies have suffered as much reputational damage as Facebook – the platform whose data was used to drive digital disinformation campaigns in the 2016 presidential election. 

But, as Samuel Wooley himself notes, change is afoot. Today, Facebook is at the forefront of efforts to get a handle on the misuse of social media and stem the tide of fake news. So what does this campaign look like on the ground? Check out our blinks to **Tools and Weapons**, an insider’s take on the company’s reforms, to find out. 
