# Tools and Weapons
*by Brad Smith and Carol Ann Browne*

Source: [https://www.blinkist.com/books/tools-and-weapons-en](https://www.blinkist.com/books/tools-and-weapons-en)

![Tools and Weapons](https://images.blinkist.com/images/books/5da5d29c6cee0700074e3418/3_4/470.jpg)

(2019) outlines the many different ways in which digital technology can both empower and endanger us. As Microsoft insiders, Brad Smith and Carol Ann Browne offer unique insight into the digital present and the future we face, from advanced AI to devastating cyberwarfare. Here they argue for a world where big tech firms and governments collaborate to ensure that the future is better for all of us.


# What’s in it for me? Get an insider’s take on the threats and promises of the digital age.

Ever feel like things are moving too fast? That the dizzying rush of technological innovation is out of control? That we should pause a moment before we embrace a world where vehicles drive themselves, shadowy data firms can swing elections, and AI can predict whether or not someone will turn into a criminal? You’re not alone. Authors Brad Smith and Carol Ann Browne of Microsoft might just agree with you. Not in the sense that we should retreat to some sort of pre-technological cave, but just that we need to get a handle on things.

To do so, governments and big tech corporations need to collaborate intelligently. By working together, they can ensure that the digital revolution brings with it the kind of technologies we urgently need: those that help us tackle our great collective challenges and make life easier for many. If they don’t work together, then we face a much darker timeline – one in which hackers can shut down hospitals, or hostile-state actors can use the internet to interfere with our democratic process.

The following blinks outline Brad Smith and Carol Ann Browne’s insights into technology today and give us a window into a future world – which could be terrifying or wonderful, depending on how we act **now**.

Along the way, you’ll find out

- why the data cloud is more like a fortress than a real cloud;
- what an eighteenth-century British MP and Edward Snowden have in common; and
- how a Stasi prison taught Microsoft a valuable lesson.

# Data has always been an integral part of human civilization.

We’ve always relied on data. All human civilizations have passed information down from one generation to the next. Without being able to record our methods, we wouldn’t have been able to make progress.

Without the scrolls of antiquity, our great architectural techniques wouldn’t have developed over centuries, mathematical solutions wouldn’t have traveled from one mind to the next, and military strategies wouldn’t have made it from Caesar’s battlefields to Napoleon’s.

Then, when Johannes Gutenberg invented the printing press, there was something of a data explosion. As more individuals gained access to the achievements of humankind through the printed word, a democratic revolution began. This had momentous consequences for religion, politics and cultural life.

Later, the acceleration of commerce in the nineteenth and twentieth centuries meant an exponential increase in the amount of data in the world. By the mid-twentieth century, there were filing cabinets overflowing with data in every organization, for every imaginable purpose.

And today, through digitization, we store a quantity of data inconceivable at any other moment in history. We call this digital architecture the **cloud**.

And though this word brings to mind a fluffy, soft cumulus floating above us, the reality is more like a fortress. The cloud has a very definite physical reality. Every time you look something up on your mobile device, you are pulling a piece of information from a gigantic data center.

These are modern marvels that almost nobody gets to see. Take the one in Quincy – a tiny town about 150 miles east of Seattle. Here, there are two campuses with more than 20 huge, nondescript buildings. Each building is the size of a football field and can comfortably house two large commercial airplanes.

At the heart of each of these buildings is a computer center, where thousands of servers are lined up in long racks. Somewhere, in one of these buildings, each of us will have our own digital file. In one of these humming, cavernous rooms, there are our photographs, private emails and bank account details.

Even more remarkable is the fact that each data center has an exact double, with another set of buildings, just like the one in Quincy, somewhere else. This way, if there’s a natural or humanmade disaster, our data – our memories, messages, private details – will be kept safe.

# Edward Snowden reignited the old question of privacy for the twenty-first century.

On 6 June 2013, the head of Microsoft’s public communications team, Dominic Carr, received an email whose contents would shock him, and then the world. 

The email, from a **Guardian** journalist, claimed that the US government’s National Security Agency (NSA) had been accessing private user data, phone records and other information belonging to millions of people, including foreign leaders around the world.

The source for this story was Edward Snowden, a 29-year-old computer systems administrator working at the NSA Threat Operation Centre in Hawaii. He’d downloaded over 1.5 million classified documents and then fled to Hong Kong before contacting the **Guardian **and **Washington Post **with his story.

What he’d revealed was that the NSA, in league with the British government, had been hacking into undersea fiber-optic cables to copy data from Yahoo and Google networks. Microsoft, whose own user info was compromised, was stunned.

At this moment, Snowden’s revelations brought about a clash between the people and their government that had deep roots. The question of how much privacy a private citizen should have has a long history, and Snowden was just the latest individual to pose it.

One of the first was John Wilkes, a British MP of the eighteenth century. He was notorious for writing critical polemics on the monarchy and the prime minister of the day. Finally, a particularly provocative letter drove the government to issue a warrant for his arrest, allowing them to search any house without warning. The law in Wilkes’ day offered little protection from trespass – the king’s soldiers could break in **anywhere** without reasonable suspicion. So, many doors were broken down, trunks ransacked and private possessions taken as proof. They arrested 49 people, almost all of whom were innocent, in their hunt for Wilkes.

Wilkes was finally arrested but decided to fight his case – and the way he was pursued – in the courts. To the establishment’s shock, he won.

As part of his case, the courts ruled that authorities must have **greater probable cause** to support a search. The British press hailed the ruling, declaring that “every Englishman’s house is his castle and is not liable to be searched.”

In many ways, Wilkes’ case marked the birth of modern privacy rights. It was an issue reignited by Edward Snowden, in 2013, when he revealed again the age-old tendency of governments to encroach on their citizens’ private lives. 

# Terrorist attacks forced tech companies to clarify their privacy policies.

As the twenty-first century unfolded, terrorist attacks became a worryingly frequent occurrence.

At first, the implications for new digital technologies weren’t obvious. 9/11, for instance, took place in a world that wasn’t yet hooked on social media, a world of fax machines rather than smartphones.

But a more recent incident, like the **Charlie Hebdo** attack in 2015, revealed a new relationship between technology and terrorism. Microsoft’s president Brad Smith was at his office in Redmond, Seattle, when he first saw the news from Paris. Two brothers, affiliated with Al-Qaeda, had entered the headquarters of the satirical magazine **Charlie Hebdo** and gunned down 12 people. Like many around the world, he was deeply disturbed by the news. However, he didn’t foresee that the attack would involve Microsoft too.

Very early the next day, the FBI, in communication with the French authorities, requested access to the terrorists’ email account details, so they could be tracked. Within 45 minutes, Smith's team at Microsoft found the account details and turned them over to the FBI. The next day, the two attackers were located using a variety of sources and IP addresses and were killed in a shootout with the police. 

A case like the **Charlie Hebdo** attack presented a clear and urgent case for granting security forces the user info they required. However, as the Edward Snowden revelations had shown, there were many private individuals and businesses who posed no immediate threat, but whose data had been accessed. And after each new terror attack, the surveillance state’s net grew tighter – governments began to demand more data on citizens who had nothing to do with terrorism.

In the United States, the government continued to ask tech companies for the details of people they were investigating. While they did this, they put in place gagging orders, laws that prohibited the tech companies from informing their customers about what was happening.

It took continued demands from the US government for data for Microsoft to take affirmative action. Dramatically, they decided to sue the government.

In the Supreme Court, the judges determined that Microsoft had a case against the government, which was that, under the First Amendment, they were within their rights to inform customers that their data was being used. This victory led the Department of Justice to sit down with the tech companies and discuss the future constructively. They decided that the gagging orders would have limits.

This was a crucial first step toward common sense – the balance between accountability **and** privacy.

# Differences in culture and history impact how different countries handle the question of data privacy.

Many of the tech innovations of the twenty-first century were developed in Silicon Valley in California. That means that they were designed with a certain cultural perspective in a country that takes freedom for granted. But what happens when this collides with different perspectives? 

One example illustrates this well.

When in Berlin for a series of meetings in 2018, the authors were taken to an old East German prison by their German Microsoft colleagues. There, they met a 75-year-old former prisoner, Hans-Jochen Scheidler, who’d been locked away in the gloomy, brutalist building for many months for delivering pamphlets criticizing the socialist regime. Like many dissidents, he’d been snared by the Stasi – the secret police. 

It was here that the author’s German colleagues pointed out a correlation between the past and present. The Stasi had kept an enormous store of data on East Germans like Scheidler, collected by a massive network of spies and citizen informers. It was one of the largest collections of information about a country’s population before the digital age.

This, the German colleagues pointed out, was why Germany was much more cautious than the United States about mass data collection. It was why, as a nation, they considered ethical questions before simply commercial interests. 

This left a deep impression on Microsoft’s president. It revealed how international data storage would come with international complications. And it meant that Microsoft would undertake a careful analysis of their data policies. They would have to review where they decided to allow data storage centers to be built.

Countries with troubling human rights records, for instance, would not be allowed to access their citizens’ data at all, while those with less autocratic, but still questionable records, would be permitted to store secondary data – that relating to businesses, for instance.

The ideal settings for big tech companies are countries with stable political circumstances and strong human rights legislation. For many years, this has been the Republic of Ireland, which, with its place in the EU, welcoming migration policy and tax incentives, has been a big draw for big tech since the 1980s.

However, as we know, the path to totalitarianism can be short, so current stability is no guarantee for the future. These are questions with which the world will grapple for decades to come. We must hope, then, that tech companies are braced.

# The world has yet to wake up to the full implications of cyberwar.

On 12 May 2017, Patrick Ward, the owner of an ice cream business, was wheeled into a surgical prep room at St Bartholomew's hospital in central London. He’d traveled three hours up to the capital from a small village in southern England for the crucial heart surgery that he’d waited two years to undergo. He lay on a gurney, wired up to monitors, with his chest shaved. A surgeon’s assistant appeared and told him that he’d have just a few more minutes to wait. 

He waited. Hours passed. Then, a doctor flung open the doors of the prep room and informed him that he wouldn’t be able to have his surgery, as the hospital’s computers were all down.

From thousands of miles away, hackers had launched a cyberattack that had rendered the whole system useless and paralyzed a third of the National Health Service.

The attack ripped through the United Kingdom and Spain before spreading to the rest of the world, impacting around three hundred thousand computers in more than 150 countries. The malware froze Windows operating systems and demanded a ransom of $300 for a password that would unlock the computer again. Aptly, the attack became known as “WannaCry” after it reduced lots of computer users to tears. 

Quite soon, it became apparent that the malware used had been developed by the United States government, but was then stolen and leaked on the dark web, possibly to a hostile state actor. In fact, it was strongly suspected that North Korea had launched the attack in retaliation against an earlier one by the United States. 

The fact that the malware was so easily stolen alarmed tech corporations like Microsoft, who stated that it was as if the American government had been careless enough to leave a stash of Tomahawk missiles lying around.

And this was no exaggeration. Though the malware was quickly countered, and people like Patrick Ward were able to have their operations, the potential consequences of more serious cyberattacks were alarming.

Imagine a future where the malware is much harder to decode – where automatic vehicles could be hacked from afar and sent spiraling out of control, where banks could be shut down, where patients could have their life support systems turned off. If we’re not alert, this is a future we’ll be looking at very soon. 

# Social media platforms have been used to sow discord in modern democracies in a way that mirrors history.

In its infancy, the internet seemed like a good way to connect the world and bring us closer, but in recent years we’ve seen its shadowy side.

Take the divisive 2016 US election, where social media platforms were used to disseminate “fake news.”

Working from St Petersburg, Russian operatives from the Internet Research Agency (IRA) created phony news stories about Hillary Clinton, hosted on wholly fake websites. These spread wildly across the internet, “seeded” by a few nodes, specific websites that would reach different types of internet users. These stories were about Clinton’s supposed ill health, her alleged connection to pedophile networks, and other similarly lurid fabrications.

On social media, these stories worked their way around certain online communities while other communities remained oblivious of them. This is how online bubbles form, with people becoming more and more polarized, sometimes believing things which have no basis in fact.

One of the most extreme consequences of this type of manipulation was the IRA’s successful attempt to organize an anti-Trump protest and a pro-Trump counter-protest in Houston, Texas, in 2016. American shouted at American, both unknowingly riled up and sent into a fury by someone sitting at a computer in St Petersburg.

Although this might look like a new threat, foreign actors have always had the ability to create discord in other nations. For instance, as Britain and France went to war in 1793, a French ambassador called Edmond Charles Genêt arrived in America just a few weeks after President George Washington had declared his nation’s neutrality. Genêt was on a mission to get the young republic to support France and immediately caused tensions in Washington’s cabinet. Before long, the French ambassador appealed directly to the American public for support, stoking a bitter divide in the population. Quite suddenly, political debate became impassioned, street brawls broke out and friendships were destroyed.

Finally, Washington’s divided cabinet came together with one objective: to send Genêt back to France before he could cause more trouble. Despite their disagreement on the Franco-British war, they resolved that no outside influence could be allowed to cause such division again. This incident provoked Washington to state in his farewell address: “A free people ought to be constantly awake, since history and experience prove that foreign influence is one of the most baneful foes of republican government.” 

In light of the Kremlin's attempt to interfere in the 2016 US election, these words carry a contemporary weight.

# AI has thrown up some complex problems.

Artificial Intelligence: what’s the first thing that comes to mind? The dark techno score and scanner gaze of **The Terminator**? R2D2? The AI romance in the film **Her**? The truth is, we’re surrounded by AI already: your smartphone learns about you as you use it, for instance.

So, what should concern us about AI today?

There is a pervasive fear that developing AI will result in all-powerful machine overlords. And now that AI is connected to the cloud – the largest store of data that has ever existed – there is an anxiety that machine learning will speed up to such an extent that a superintelligence will emerge. The amalgamation of all this data would be called the **Singularity**, and it would reproduce increasingly sophisticated AI. 

However, **so far**, this is a science fiction fantasy. There are real concerns with AI in the here and now, and they say much more about the human beings that build the computers than the technology itself.

The central concern, in fact, is the bias that is present in AI. For instance, at a tech conference at the White House in 2016, all the attention turned to an article in the magazine **ProPublica** titled “Machine Bias.” The article’s subtitle explained the concern: “There’s software used across the country to predict future criminals. And it’s biased against blacks.”

This bias existed because of the problem of unrepresentative data sets. For instance, take facial recognition technology. A facial recognition data set might include enough photos of white males to predict, with a high accuracy rate, the faces of white men. But if there are smaller sets of women or people of color, then more errors with these demographics are likely.

A similar conclusion to the **ProPublica **article was found in research by the poet and scholar Joy Buolamwini and Stanford University researcher Timnit Gebru. In their study of facial recognition technology, they found worse accuracy rates for black politicians in Africa compared to white politicians in Europe.

Crucially, their study unearthed another dimension of bias. This entailed the teams that build new technologies. They found that, unless tech teams reflected the diversity of the world, it was very likely that their inventions would contain prejudicial blind spots. 

What’s more, they found that a more diverse group of researchers and engineers was more likely to spot problems in the design, as bias was something that affected them **personally**.

# New technologies can be used constructively, but joined-up thinking is required.

A tool can be used for good or evil. With a broom, you can sweep a kitchen floor or smash someone over the head. The same applies to information technology.

Consider these recent innovations:

At Princeton College, Marina Rustow, a professor of Near Eastern studies, is working on decoding a trove of four hundred thousand documents from Cairo’s Ben Ezra Synagogue. It’s the largest recorded cache of Jewish manuscripts in the world. Of course, studying these documents is a formidable challenge – many of them are in fragments or scattered in archives around the world. This makes physically piecing them together impossible.

However, using advanced AI, Rustow’s team has been able to match digital fragments that are thousands of miles apart with a speed and precision that no human being could manage. This has meant Rustow has been able to understand a previously little-known part of the Middle Ages, during which Jews and Muslims coexisted peacefully.

Similarly, AI can be used to preserve the living world. Microsoft’s AI for Earth team has developed a program that helps park rangers in Uganda ward off poachers. By using an algorithm, the rangers can predict poaching behavior, which helps them **proactively **identify poaching hot spots.

These are just two ways in which technology can be put to beneficial use. However, if we’re to make this the aim going forward, big tech and governments must collaborate more.

Firstly, tech companies can no longer see themselves as unaccountable to the world. There needs to be a broader consideration of aims, beyond purely commercial interests. Rather than simply competing with other firms, the heads of big tech – as people who wield enormous power – need joined-up thinking around their moral obligations.

Secondly, governments must understand and regulate technology if this world is to become reality. Government officials are obligated to educate themselves on emerging tech. No longer is it permissible for officials to be ignorant of something so transformative – like the US senator the authors describe, who wasn’t aware that he could read the **Washington Post** online. And in terms of regulation – nobody would consider it right for the aviation industry to be unregulated, so why should something as high-risk as digital technology go unchecked? 

So, this is the outline for the future – a world where we master technology for positive ends, or one where it masters us. We need to choose – quick. 

# Final summary

The key message in these blinks:

**New digital technologies present us with astonishing possibilities but also with unforeseen threats. We can either master these inventions for good, such as by using AI to fight poaching and climate change, or we can allow their darker potential to be harnessed by hostile actors. To ensure that technology is a force for good, it is vital that tech companies collaborate with governments on regulation and an ethical framework.**

Actionable advice:

**When you read a news story, make sure it can be verified.**

When browsing the internet and coming across some particularly inflammatory headline, check that it’s grounded in truth. Here’s what you can do. See if numerous, well-regarded sources repeat the same story. If not, and it’s limited to only one source, be careful that you’re not being hoodwinked! 

**Got feedback?**

We’d sure love to hear what you think about our content! Just drop an email to remember@blinkist.com with the title of this book as the subject line and share your thoughts!

**What to read next: ******Hit Refresh******, by Satya Nadella****** ****

You’ve just heard from those on the inside of Microsoft’s operations about what technology promises for the future. That future can either be one of positive change or a terrifying dystopia, depending on the ability of tech companies and governments to get a handle on things.

Now, it’s time to go even deeper into the machine with Satya Nadella, the CEO of Microsoft. Reimagining the corporation for the twenty-first century, Nadella has been something of a revelation in the world of business and big tech. Get the blinks to **Hit Refresh** to find out more about Nadella’s dreams for the company and his crusade to make a better future for everyone. 
